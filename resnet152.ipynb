{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"resnet152.ipynb","private_outputs":true,"provenance":[{"file_id":"12lcPm24-ziuGVP-0UwG7z7kcFPNxlKqz","timestamp":1616415669105}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"code","metadata":{"id":"fZIOt7QfS7TR"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-c1_rzXS7NR"},"source":["#경로 설정\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/모션키포인트검출AI경진대회')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqxrdSx1yoxz"},"source":["# Motion Keypoint Baseline"]},{"cell_type":"markdown","metadata":{"id":"6DWSjEsmysjq"},"source":["### Module Mount & Data Load"]},{"cell_type":"code","metadata":{"id":"3Igb6uxYylZN"},"source":["import cv2\n","import glob\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","random.seed(2021)\n","import shutil\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import LeakyReLU, PReLU\n","from math import cos, sin, pi\n","from PIL import Image\n","from tqdm import tqdm\n","from tensorflow.keras import Sequential, Model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import Activation, Convolution2D, MaxPool2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, Conv2DTranspose, Input\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7xqzmKCy1ed"},"source":["### Train, Valid Split"]},{"cell_type":"code","metadata":{"id":"MsMnpgAHS3Ys"},"source":["# # train, val folder 생성\n","# root_dir = './data'\n","\n","# os.makedirs(root_dir +'/train')\n","# os.makedirs(root_dir +'/valid')\n","\n","# # validation용 파일은 10% 비율로 random sampling\n","# # random.seed() 넣으시면 복원 가능\n","# src = \"data/train_imgs\"\n","# all_filename = os.listdir(src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EdzoQYbXS3Ys"},"source":["# valid_filename = random.sample(all_filename, int(len(all_filename) * 0.1))\n","# train_filename = [x for x in all_filename if x not in valid_filename]\n","\n","# print(len(train_filename), len(valid_filename))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BbG_haXy5ia"},"source":["# train_filename = [src+'/'+ name for name in train_filename]\n","# valid_filename = [src+'/' + name for name in valid_filename]\n","\n","# # copy & paste images\n","# for name in tqdm(train_filename):\n","#     shutil.copy(name, 'data/train')\n","\n","# for name in tqdm(valid_filename):\n","#     shutil.copy(name, 'data/valid')\n","    \n","# print('Total images: ', len(all_filename))\n","# print('Training: ', len(train_filename))\n","# print('Validation: ', len(valid_filename))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRisfwtNy7Cm"},"source":["# # train, valid folder 속 모든 이미지 파일 read & sort\n","# train_paths = glob.glob('./data/train/*.jpg')\n","# valid_paths = glob.glob('./data/valid/*.jpg')\n","# train_paths.sort()\n","# valid_paths.sort()\n","\n","# train_filename = []\n","# valid_filename = []\n","\n","# for t_paths in tqdm(train_paths):\n","#     filename = t_paths.split('/')[-1].split('\\\\')[1]\n","#     train_filename.append(filename)\n","\n","# for v_paths in tqdm(valid_paths):\n","#     filename = v_paths.split('/')[-1].split('\\\\')[1]\n","#     valid_filename.append(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OY8OPlUy89l"},"source":["# # 각각의 train, valid 이미지들의 정보만을 담고 있는 DataFrame 생성\n","# train = pd.read_csv('data/train_df.csv')\n","# train_df = train[train['image'].isin(train_filename)]\n","# train_df.reset_index(inplace=True, drop=True)\n","\n","# valid_df = train[train['image'].isin(valid_filename)]\n","# valid_df.reset_index(inplace=True, drop=True)\n","\n","# train_df.to_csv('data/train.csv', index=False)\n","# valid_df.to_csv('data/valid.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qxj-5bEUS3Yt"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"GjDTwaOtS3Yt"},"source":["# 해당 코드는 아래 Train, Valid Split 이후에 실행\n","train = pd.read_csv('./data/train.csv')\n","valid = pd.read_csv('./data/valid.csv')\n","\n","train_paths = glob.glob('./data/train/*.jpg')\n","valid_paths = glob.glob('./data/valid/*.jpg')\n","test_paths = glob.glob('./data/test_imgs/*.jpg')\n","print(len(train_paths), len(valid_paths), len(test_paths))\n","\n","train_paths.sort()\n","valid_paths.sort()\n","test_paths.sort()\n","\n","train['path'] = train_paths\n","valid['path'] = valid_paths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fA5h-wovS3Yt"},"source":["## 시각화"]},{"cell_type":"code","metadata":{"id":"rSfWYyTWS3Yt"},"source":["plt.figure(figsize=(40,20))\n","count=1\n","\n","for i in np.random.randint(0,len(train_paths),5):\n","    \n","    plt.subplot(5,1, count)\n","    \n","    img_sample_path = train_paths[i]\n","    img = Image.open(img_sample_path)\n","    img_np = np.array(img)\n","\n","    keypoint = train.iloc[:,1:49] #위치 키포인트 하나씩 확인\n","    keypoint_sample = keypoint.iloc[i, :]\n","    \n","    for j in range(0,len(keypoint.columns),2):\n","        plt.plot(keypoint_sample[j], keypoint_sample[j+1],'rx')\n","        plt.imshow(img_np)\n","    \n","    count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVQe-V0xzBVX"},"source":["### Augmentation"]},{"cell_type":"code","metadata":{"id":"slvHoEeVy_kU"},"source":["# Augmentation Setting\n","pixel_shifts = [12]\n","rotation_angles = [12]\n","inc_brightness_ratio = 1.2\n","dec_brightness_ratio = 0.8\n","noise_ratio = 0.008"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuQc7OH-zDHV"},"source":["# 좌우 반전\n","def left_right_flip(images, keypoints):\n","    flipped_keypoints = []\n","    flipped_images = np.flip(images, axis=1)\n","    for idx, sample_keypoints in enumerate(keypoints):\n","        if idx%2 == 0:\n","            flipped_keypoints.append(480.-sample_keypoints)\n","        else:\n","            flipped_keypoints.append(sample_keypoints)\n","    \n","    # left_right_keypoints_convert\n","    for i in range(8):\n","        flipped_keypoints[2+(4*i):4+(4*i)], flipped_keypoints[4+(4*i):6+(4*i)] = flipped_keypoints[4+(4*i):6+(4*i)], flipped_keypoints[2+(4*i):4+(4*i)]\n","    flipped_keypoints[36:38], flipped_keypoints[38:40] = flipped_keypoints[38:40], flipped_keypoints[36:38]\n","    flipped_keypoints[44:46], flipped_keypoints[46:48] = flipped_keypoints[46:48], flipped_keypoints[44:46]\n","    \n","    return flipped_images, flipped_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EIf0fxQzEvf"},"source":["# 수직/수평 동시 이동\n","# forloop에서 shift_x, shift_y 중 하나만 놓으면\n","# 수직 또는 수평 이동만 따로 시행 가능\n","def shift_images(images, keypoints):\n","    # tensor -> numpy\n","    images = images.numpy()\n","    shifted_images = []\n","    shifted_keypoints = []\n","    for shift in pixel_shifts:   \n","        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n","            # 이동할 matrix 생성\n","            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n","            shifted_keypoint = np.array([])\n","            shifted_x_list = np.array([])\n","            shifted_y_list = np.array([])\n","            # 이미지 이동\n","            shifted_image = cv2.warpAffine(images, M, (480,270), flags=cv2.INTER_CUBIC)\n","            # 이동한만큼 keypoint 수정\n","            for idx, point in enumerate(keypoints):\n","                if idx%2 == 0: \n","                    shifted_keypoint = np.append(shifted_keypoint, point+shift_x)\n","                    shifted_x_list = np.append(shifted_x_list, point+shift_x)\n","                else: \n","                    shifted_keypoint =np.append(shifted_keypoint, point+shift_y)\n","                    shifted_y_list = np.append(shifted_y_list, point+shift_y)\n","            # 수정된 keypoint가 이미지 사이즈를 벗어나지 않으면 append\n","            if np.all(0.0<shifted_x_list) and np.all(shifted_x_list<480) and np.all(0.0<shifted_y_list) and np.all(shifted_y_list<270):\n","                shifted_images.append(shifted_image.reshape(270,480,3))\n","                shifted_keypoints.append(shifted_keypoint)\n","\n","    return shifted_images, shifted_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBPEXH-QzF1P"},"source":["# 이미지 회전\n","def rotate_augmentation(images, keypoints):\n","    # tensor -> numpy\n","    images = images.numpy()\n","    rotated_images = []\n","    rotated_keypoints = []\n","    \n","    for angle in rotation_angles:\n","        for angle in [angle,-angle]:\n","            # 회전할 matrix 생성\n","            M = cv2.getRotationMatrix2D((240,135), angle, 1.0)\n","            # cv2_imshow로는 문제없지만 추후 plt.imshow로 사진을 확인할 경우 black screen 생성...\n","            # 혹시 몰라 matrix를 ndarray로 변환\n","            M = np.array(M, dtype=np.float32)\n","            angle_rad = -angle*pi/180\n","            rotated_image = cv2.warpAffine(images, M, (480,270))\n","            rotated_images.append(rotated_image)\n","            \n","            # keypoint를 copy하여 forloop상에서 값이 계속 없데이트 되는 것을 회피\n","            rotated_keypoint = keypoints.copy()\n","            rotated_keypoint[0::2] = rotated_keypoint[0::2] - 240\n","            rotated_keypoint[1::2] = rotated_keypoint[1::2] - 135\n","            \n","            for idx in range(0,len(rotated_keypoint),2):\n","                rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n","                rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n","\n","            rotated_keypoint[0::2] = rotated_keypoint[0::2] + 240\n","            rotated_keypoint[1::2] = rotated_keypoint[1::2] + 135\n","            rotated_keypoints.append(rotated_keypoint)\n","        \n","    return rotated_images, rotated_keypoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7oC3mNHzH0d"},"source":["# 이미지 해상도 조절\n","def alter_brightness(images):\n","    altered_brightness_images = []\n","    inc_brightness_images = np.clip(images*inc_brightness_ratio, 0.0, 1.0)\n","    dec_brightness_images = np.clip(images*dec_brightness_ratio, 0.0, 1.0)\n","    altered_brightness_images.append(inc_brightness_images)\n","    altered_brightness_images.append(dec_brightness_images)\n","    return altered_brightness_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJTY9OnmzJZT"},"source":["# Random 노이즈 추가\n","def add_noise(images):\n","    images = images.numpy()\n","    noise = noise_ratio * np.random.randn(270,480,3)\n","    noise = noise.astype(np.float32)\n","    # 생성한 noise를 원본에 add\n","    noisy_image = cv2.add(images, noise)\n","    return noisy_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcLJSpTBzQIq"},"source":["### Generator"]},{"cell_type":"code","metadata":{"id":"-jN-xAAZzNAp"},"source":["def trainGenerator():\n","    # 원본 이미지 resize\n","    for i in range(len(train)):\n","        img = tf.io.read_file(train['path'][i]) # path(경로)를 통해 이미지 읽기\n","        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n","        img = tf.image.resize(img, [270,480]) # 이미지 resize \n","        img = img/255                         # 이미지 rescaling\n","        target = train.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n","        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n","\n","        yield (img, target)\n","    \n","    # horizontal flip\n","    for i in range(len(train)):\n","        img = tf.io.read_file(train['path'][i]) \n","        img = tf.image.decode_jpeg(img, channels=3) \n","        img = tf.image.resize(img, [270,480]) \n","        img = img/255\n","        target = train.iloc[:,1:49].iloc[i,:] \n","        target = target/4\n","        img, target = left_right_flip(img, target)\n","        \n","        yield (img, target)\n","\n","    # # Horizontal & Vertical shift\n","    # for i in range(len(train)):\n","    #     img = tf.io.read_file(train['path'][i])\n","    #     img = tf.image.decode_jpeg(img, channels=3)\n","    #     img = tf.image.resize(img, [270,480])\n","    #     img = img/255\n","    #     target = train.iloc[:,1:49].iloc[i,:]\n","    #     target = target/4\n","    #     img_list, target_list = shift_images(img, target)\n","    #     for shifted_img, shifted_target in zip(img_list, target_list):\n","            \n","    #         yield (shifted_img, shifted_target)\n","\n","    # # Rotation\n","    # for i in range(len(train)):\n","    #     img = tf.io.read_file(train['path'][i])\n","    #     img = tf.image.decode_jpeg(img, channels=3)\n","    #     img = tf.image.resize(img, [270,480])\n","    #     img = img/255\n","    #     target = train.iloc[:,1:49].iloc[i,:]\n","    #     target = target/4\n","    #     img_list, target_list = rotate_augmentation(img, target)\n","    #     for rotated_img, rotated_target in zip(img_list, target_list):\n","            \n","    #         yield (rotated_img, rotated_target)\n","\n","    # # Alter_Brightness\n","    # for i in range(len(train)):\n","    #     img = tf.io.read_file(train['path'][i])\n","    #     img = tf.image.decode_jpeg(img, channels=3)\n","    #     img = tf.image.resize(img, [270,480])\n","    #     img = img/255\n","    #     target = train.iloc[:,1:49].iloc[i,:]\n","    #     target = target/4\n","    #     img_list = alter_brightness(img)\n","    #     for altered_brightness_images in img_list:\n","            \n","    #         yield (altered_brightness_images, target)\n","\n","    # # Adding_Noise\n","    # for i in range(len(train)):\n","    #     img = tf.io.read_file(train['path'][i])\n","    #     img = tf.image.decode_jpeg(img, channels=3)\n","    #     img = tf.image.resize(img, [270,480])\n","    #     img = img/255\n","    #     target = train.iloc[:,1:49].iloc[i,:]\n","    #     target = target/4\n","    #     noisy_img = add_noise(img)\n","\n","    #     yield (noisy_img, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNN_5hfszUKE"},"source":["def validGenerator():\n","    # 원본 이미지 resize\n","    for i in range(len(valid)):\n","        img = tf.io.read_file(valid['path'][i]) # path(경로)를 통해 이미지 읽기\n","        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\n","        img = tf.image.resize(img, [270,480]) # 이미지 resize \n","        img = img/255                         # 이미지 rescaling\n","        target = valid.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\n","        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\n","\n","        yield (img, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QLgYDKdzVsU"},"source":["batch_size = 16\n","\n","train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n","train_dataset = train_dataset.batch(batch_size).prefetch(1)\n","valid_dataset = tf.data.Dataset.from_generator(validGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\n","valid_dataset = valid_dataset.batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UWg9YNvAzkuP"},"source":["### Pre-trained Model"]},{"cell_type":"code","metadata":{"id":"4m5YkthhzjoS"},"source":["# tensorflow.keras.applications 사용 예시\n","from tensorflow.keras import models\n","from tensorflow.keras.applications import ResNet152V2\n","\n","earlystop = EarlyStopping(patience=2)\n","# learning_rate_reduction = ReduceLROnPlateau(\n","#                         monitor= \"val_loss\", \n","#                         patience = 2, \n","#                         factor = 0.85, \n","#                         min_lr = 1e-7,\n","#                         verbose = 1)\n","\n","model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \n","        filepath=\"./resnet152.h5\", #모델 파일 경로\n","        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n","        save_best_only=True)\n","\n","callbacks = [earlystop, model_check]\n","\n","\n","\n","# https://paperswithcode.com/paper/evopose2d-pushing-the-boundaries-of-2d-human\n","# https://arxiv.org/pdf/2011.08446v1.pdf\n","# https://github.com/wmcnally/evopose2d\n","# https://www.tensorflow.org/api_docs/python/tf/keras/applications"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI8mRFsFd2KM"},"source":["# resnet152\n","resnet152 = ResNet152V2(weights ='imagenet', include_top = False, input_shape = (270,480,3))\n","\n","for layer in resnet152.layers:\n","    layer.trainable = True\n","\n","# Building Layers\n","\n","model = Sequential()\n","model.add(resnet152)\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Convolution2D(2048, (3,3), padding='same', use_bias=False))\n","model.add(LeakyReLU(alpha = 0.1))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Convolution2D(4096, (3,3), padding='same', use_bias=False))\n","model.add(LeakyReLU(alpha = 0.1))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.1))\n","model.add(Dense(256, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.1))\n","model.add(Dense(48))\n","\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vHQFsBu1Nxy"},"source":["model.compile(loss='mean_squared_error',\n","                optimizer='adam',\n","                metrics=['mae'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"gDklJnNmS3Yw"},"source":["# model.load_weights('resnet152.h5')\n","history = model.fit(train_dataset,\n","                    epochs=100,\n","                    validation_data=valid_dataset,\n","                    callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yqWlwWUuz09O"},"source":["### Load TestSet & Predict"]},{"cell_type":"code","metadata":{"id":"xv_YAgEKz0mB"},"source":["test_paths = glob.glob('./data/test_imgs/*.jpg')\n","test_paths.sort()\n","X_test=[]\n","\n","for test_path in tqdm(test_paths):\n","    img=tf.io.read_file(test_path)\n","    img=tf.image.decode_jpeg(img, channels=3)\n","    img=tf.image.resize(img, [270,480])\n","    img=img/255\n","    X_test.append(img)\n","\n","X_test=tf.stack(X_test, axis=0)\n","X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJOpC-mX0BNR"},"source":["model.load_weights('resnet152.h5')\n","pred = model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_u95-gO0DaT"},"source":["### Submission"]},{"cell_type":"code","metadata":{"id":"d0bx89ky0ErL"},"source":["submission = pd.read_csv('./data/sample_submission.csv')\n","submission.iloc[:,1:] = pred * 4     # image size를 1920x1080 -> 480x270으로 바꿔서 예측했으므로 * 4\n","# submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ps6i8sV0WUV"},"source":["submission.to_csv('resnet152.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_4HAZqu3kP0"},"source":["### 예측 결과 시각화"]},{"cell_type":"code","metadata":{"id":"WbBr-hji0bdo"},"source":["# 예측 결과 시각화\n","n = random.randint(0, 1600)\n","predicted_keypoint = submission.iloc[n,1:49]\n","predicted_keypoint = np.array(predicted_keypoint)\n","img = Image.open(test_paths[n])\n","plt.imshow(img)\n","plt.scatter(predicted_keypoint[0::2], predicted_keypoint[1::2], marker='x')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnlKSkAwS3Yx"},"source":["# https://dacon.io/competitions/official/235701/codeshare/2383?page=1&dtype=recent&ptype=pub\n","# https://www.kaggle.com/gauravrajpal/facial-keypoint-detection-vgg16"],"execution_count":null,"outputs":[]}]}